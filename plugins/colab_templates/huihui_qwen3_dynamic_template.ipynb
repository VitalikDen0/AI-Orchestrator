{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c174ba1b",
   "metadata": {},
   "source": [
    "# ü§ñ HuiHui-Qwen3-4B –¥–ª—è AI Orchestrator\n",
    "\n",
    "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å HuiHui-Qwen3-4B-Thinking –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI Orchestrator —á–µ—Ä–µ–∑ —É–¥–∞–ª–µ–Ω–Ω—ã–π API.\n",
    "\n",
    "## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏\n",
    "- ‚ö° GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ Google Colab\n",
    "- üß† –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "- üåê API —Å–µ—Ä–≤–µ—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞\n",
    "- üîó –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —á–µ—Ä–µ–∑ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03669f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–±—É–¥–µ—Ç –ø–æ–ª—É—á–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)\n",
    "SYSTEM_PROMPT = ''\n",
    "\n",
    "def update_system_prompt(new_prompt):\n",
    "    \"\"\"–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞\"\"\"\n",
    "    global SYSTEM_PROMPT\n",
    "    SYSTEM_PROMPT = new_prompt\n",
    "    print(f'üîÑ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {len(SYSTEM_PROMPT)} —Å–∏–º–≤–æ–ª–æ–≤')\n",
    "\n",
    "print('üìù –ì–æ—Ç–æ–≤ –∫ –ø–æ–ª—É—á–µ–Ω–∏—é —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —á–µ—Ä–µ–∑ API')\n",
    "print(f'üìè –¢–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç: {len(SYSTEM_PROMPT)} —Å–∏–º–≤–æ–ª–æ–≤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è HuiHui-Qwen3\n",
    "!pip install transformers torch accelerate bitsandbytes\n",
    "!pip install flask ngrok-python requests\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "\n",
    "print(f'üî• GPU –¥–æ—Å—Ç—É–ø–Ω–æ: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üñ•Ô∏è GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'üíæ GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ HuiHui-Qwen3-4B –º–æ–¥–µ–ª–∏\n",
    "model_name = \"HuiHui/Huihui-Qwen3-4B-Thinking-2507-abliterated\"\n",
    "\n",
    "print(f'üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}')\n",
    "print('‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...')\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print('‚úÖ –ú–æ–¥–µ–ª—å HuiHui-Qwen3 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!')\n",
    "print(f'üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: ~4B')\n",
    "print(f'üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated()/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09098409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "def generate_response(prompt, max_tokens=2048, temperature=0.7, use_system_prompt=True):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è HuiHui-Qwen3 –º–æ–¥–µ–ª—å\"\"\"\n",
    "    try:\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "        if use_system_prompt and SYSTEM_PROMPT:\n",
    "            full_prompt = f\"{SYSTEM_PROMPT}\\n\\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {prompt}\\n\\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:\"\n",
    "        else:\n",
    "            full_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        inputs = tokenizer(\n",
    "            full_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=4096\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'response': response,\n",
    "            'tokens_used': outputs[0].shape[1],\n",
    "            'model': 'huihui-qwen3-4b-colab',\n",
    "            'system_prompt_used': use_system_prompt and bool(SYSTEM_PROMPT)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'model': 'huihui-qwen3-4b-colab'\n",
    "        }\n",
    "\n",
    "print('‚úÖ API —Ñ—É–Ω–∫—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4887c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API —Å–µ—Ä–≤–µ—Ä –¥–ª—è –ø—Ä–∏–µ–º–∞ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model': 'huihui-qwen3-4b-colab',\n",
    "        'gpu_available': torch.cuda.is_available(),\n",
    "        'memory_used': f\"{torch.cuda.memory_allocated()/1024**3:.1f} GB\" if torch.cuda.is_available() else \"N/A\",\n",
    "        'system_prompt_loaded': bool(SYSTEM_PROMPT),\n",
    "        'system_prompt_length': len(SYSTEM_PROMPT)\n",
    "    })\n",
    "\n",
    "@app.route('/update_prompt', methods=['POST'])\n",
    "def update_prompt():\n",
    "    \"\"\"–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        new_prompt = data.get('system_prompt', '')\n",
    "        \n",
    "        update_system_prompt(new_prompt)\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'message': '–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω',\n",
    "            'prompt_length': len(SYSTEM_PROMPT)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_text():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        prompt = data.get('prompt', '')\n",
    "        max_tokens = data.get('max_tokens', 2048)\n",
    "        temperature = data.get('temperature', 0.7)\n",
    "        use_system_prompt = data.get('use_system_prompt', True)\n",
    "        \n",
    "        # –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –µ—Å—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º –µ–≥–æ\n",
    "        if 'system_prompt' in data and data['system_prompt']:\n",
    "            update_system_prompt(data['system_prompt'])\n",
    "        \n",
    "        if not prompt:\n",
    "            return jsonify({'status': 'error', 'error': 'Prompt is required'}), 400\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "        result = generate_response(\n",
    "            prompt=prompt, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature, \n",
    "            use_system_prompt=use_system_prompt\n",
    "        )\n",
    "        \n",
    "        return jsonify(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "print('üåê Flask API —Å–µ—Ä–≤–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω!')\n",
    "print('üì° –î–æ—Å—Ç—É–ø–Ω—ã–µ endpoints:')\n",
    "print('  GET  /health - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è')\n",
    "print('  POST /update_prompt - –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞')\n",
    "print('  POST /generate - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ngrok —Ç—É–Ω–Ω–µ–ª—è –¥–ª—è –≤–Ω–µ—à–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω ngrok (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "# ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º Flask –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# –î–∞–µ–º –≤—Ä–µ–º—è Flask –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è\n",
    "time.sleep(3)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º ngrok —Ç—É–Ω–Ω–µ–ª—å\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f'üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω!')\n",
    "print(f'üîó –ü—É–±–ª–∏—á–Ω—ã–π URL: {public_url}')\n",
    "print(f'üîó Health check: {public_url}/health')\n",
    "print(f'üîó Update prompt: {public_url}/update_prompt')\n",
    "print(f'üîó Generate endpoint: {public_url}/generate')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º URL –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "with open('colab_server_url.txt', 'w') as f:\n",
    "    f.write(str(public_url))\n",
    "\n",
    "print('\\n‚úÖ HuiHui-Qwen3 –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–∏–µ–º—É –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç AI Orchestrator!')\n",
    "print('üìã –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ø—É–±–ª–∏—á–Ω—ã–π URL –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –≤ –ø–ª–∞–≥–∏–Ω–µ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è.')\n",
    "print(f'üß† –ì–æ—Ç–æ–≤ –∫ –ø–æ–ª—É—á–µ–Ω–∏—é —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —á–µ—Ä–µ–∑: {public_url}/update_prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56623da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ endpoints\n",
    "import time\n",
    "time.sleep(2)  # –î–∞–µ–º –≤—Ä–µ–º—è —Å–µ—Ä–≤–µ—Ä—É –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ngrok\n",
    "try:\n",
    "    base_url = str(public_url)\n",
    "    \n",
    "    # –¢–µ—Å—Ç health check\n",
    "    print(\"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º health check...\")\n",
    "    health_response = requests.get(f\"{base_url}/health\")\n",
    "    if health_response.status_code == 200:\n",
    "        health_data = health_response.json()\n",
    "        print(f\"‚úÖ Health check —É—Å–ø–µ—à–µ–Ω: {health_data['status']}\")\n",
    "        print(f\"üìä GPU –¥–æ—Å—Ç—É–ø–Ω–æ: {health_data['gpu_available']}\")\n",
    "        print(f\"üíæ –ü–∞–º—è—Ç—å: {health_data['memory_used']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Health check –Ω–µ—É–¥–∞—á–µ–Ω: {health_response.status_code}\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞\n",
    "    print(\"\\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞...\")\n",
    "    test_prompt = \"–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ Google Colab.\"\n",
    "    prompt_response = requests.post(f\"{base_url}/update_prompt\", \n",
    "                                  json={\"system_prompt\": test_prompt})\n",
    "    if prompt_response.status_code == 200:\n",
    "        prompt_data = prompt_response.json()\n",
    "        print(f\"‚úÖ –ü—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {prompt_data['prompt_length']} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    else:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞: {prompt_response.status_code}\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–µ–±–æ–ª—å—à–æ–π)\n",
    "    print(\"\\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...\")\n",
    "    gen_response = requests.post(f\"{base_url}/generate\", \n",
    "                               json={\n",
    "                                   \"prompt\": \"–ü—Ä–∏–≤–µ—Ç!\", \n",
    "                                   \"max_tokens\": 50,\n",
    "                                   \"temperature\": 0.7\n",
    "                               })\n",
    "    if gen_response.status_code == 200:\n",
    "        gen_data = gen_response.json()\n",
    "        if gen_data['status'] == 'success':\n",
    "            print(f\"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞: {gen_data['response'][:50]}...\")\n",
    "            print(f\"üß† –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {gen_data['system_prompt_used']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_data['error']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
    "\n",
    "print(\"\\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c511e",
   "metadata": {},
   "source": [
    "## üéØ –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!\n",
    "\n",
    "–ú–æ–¥–µ–ª—å HuiHui-Qwen3-4B –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ API —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω.\n",
    "\n",
    "**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
    "1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ø—É–±–ª–∏—á–Ω—ã–π URL –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–µ–∫\n",
    "2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ—Ç URL –≤ AI Orchestrator –ø–ª–∞–≥–∏–Ω–µ\n",
    "3. –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ API\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:**\n",
    "```json\n",
    "POST /update_prompt\n",
    "{\n",
    "  \"system_prompt\": \"–¢–≤–æ–π –Ω–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º:**\n",
    "```json\n",
    "POST /generate\n",
    "{\n",
    "  \"prompt\": \"–†–∞—Å—Å–∫–∞–∂–∏ –æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ\",\n",
    "  \"max_tokens\": 512,\n",
    "  \"temperature\": 0.7,\n",
    "  \"system_prompt\": \"–ü—Ä–æ–º–ø—Ç –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏ –∑–¥–µ—Å—å\",\n",
    "  \"use_system_prompt\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:**\n",
    "- ‚úÖ –ú–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø—Ä–æ–º–ø—Ç –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "- ‚úÖ –ü–µ—Ä–µ–¥–∞—á–∞ —á–µ—Ä–µ–∑ –∑–∞—â–∏—â–µ–Ω–Ω—ã–π ngrok —Ç—É–Ω–Ω–µ–ª—å\n",
    "- ‚úÖ –û—Ç–¥–µ–ª—å–Ω—ã–π endpoint –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "- ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —á–µ—Ä–µ–∑ /health endpoint"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
